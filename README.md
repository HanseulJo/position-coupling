# Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure

Github repository for:
* Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, and Chulhee Yun. "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure." NeurIPS 2024. ðŸ¥³ [arxiv.org/abs/2405.20671](https://arxiv.org/abs/2405.20671)
* Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, and Chulhee Yun. "Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count." arXiv preprint. [arxiv.org/abs/2410.15787](https://arxiv.org/abs/2410.15787)


## Citations

```bibtex
@inproceedings{cho2024position,
    title={Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure}, 
    author={Hanseul Cho and Jaeyoung Cha and Pranjal Awasthi and Srinadh Bhojanapalli and Anupam Gupta and Chulhee Yun},
    booktitle={Advances in Neural Information Processing Systems},
    volume={38},
    year={2024}
}

@article{cho2024arithmetic,
    title={Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count}, 
    author={Hanseul Cho and Jaeyoung Cha and Srinadh Bhojanapalli and Chulhee Yun},
    journal={arXiv preprint arXiv:2410.15787},
    year={2024},
}
```

## Conda Environment Setting

Minimal environment to run our code base:

```bash
conda env create -f env.yaml
```

## How to run our codes

If you want to train a single model with a single combination of random seeds, you may run `run.py`. Use `--override` to change the model/task/training configurations as you want.

```bash
python run.py \
    --override \
        use_wandb=True \
        group_name="<GroupName>" \
        exp_name="<ExperimentName>" \
        seed=999 \
        seed_data=42 \
        model="CustomT5DecoderOnly" \
        model.position_encoding_type="abs_learned" \
        model.num_layers=6 \
        model.num_heads=8 \
        model.save=True \
        task="addition_coupled" \
        task.max_position=102 \
        task.train.n_data=1000000 \
        training="default" \
        training.batch_size_train=1000 \
        training.batch_size_eval=100 \
        training.n_steps=50000 \
        training.optimizer.lr=0.0001
```

The result will be logged in the `log/` directory. An example of the file structure of the logging directory is as follows:

```
log/
â””â”€â”€ <GroupName>
    â””â”€â”€ <ExperimentName>
        â””â”€â”€ seed999_seedData42
            â”œâ”€â”€ cfg.json 
            â”œâ”€â”€ best_<MODEL_NAME>.pt
            â”œâ”€â”€ last_<MODEL_NAME>.pt
            â”œâ”€â”€ loss.pdf
            â”œâ”€â”€ instancewise_accuracy.pdf
            â””â”€â”€ tokenwise_accuracy.pdf
```

If you have multiple number of devices (e.g., GPUs), we highly recommend you to run `run_parallel.py` to train the models with exactly the same configuration but with different combinations of random seeds.

```bash
python run_parallel.py \
    --use_wandb \
    --group_name "<GroupName>" \
    --exp_name "<ExperimentName>" \
    --seeds 0 1 2 3 \
    --seeds_data 0 1 \
    --devices 0 1 2 3 \
    --num_exp_per_device 2 \
    --override \
        model="CustomT5DecoderOnly" \
        model.position_encoding_type="abs_learned" \
        model.num_layers=6 \
        model.num_heads=8 \
        model.save=True \
        task="addition_coupled" \
        task.max_position=102 \
        task.train.n_data=1000000 \
        training="default" \
        training.batch_size_train=1000 \
        training.batch_size_eval=100 \
        training.n_steps=50000 \
        training.optimizer.lr=0.0001
```

For more examples of running codes, please check `scripts/` directory.


## Remarks

* Our modeling codes (e.g., `CustomT5DecoderOnly`) are mostly based on the modification by [this repository](https://github.com/McGill-NLP/length-generalization). 
    - Our code basically supports various positional embedding (PE) schemes such as Rotary PE, T5's relative bias, Alibi, Absolute Fixed PE, etc. We also manually implemented [FIRE](https://openreview.net/forum?id=rR03qFesqk). However, they are not tested except for NoPE (`model.position_encoding_type="none"`) and Absolute Learned PE (`model.position_encoding_type="abs_learned"`).
* We use [Hydra](https://hydra.cc) to maintain the configurations.


## File Structure
```
.
â”œâ”€â”€ attention_matrix.py     (only for `CustomT5DecoderOnly` model)
â”œâ”€â”€ env.yaml                (Conda environment)
â”œâ”€â”€ evaluate_model.py       (model evaluation)
â”œâ”€â”€ run.py
â”œâ”€â”€ run_parallel.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ CustomT5DecoderOnly.yaml
â”‚   â”‚   â””â”€â”€ ... other model configs ...
â”‚   â”œâ”€â”€ task/
â”‚   â”‚   â”œâ”€â”€ addition.yaml
â”‚   â”‚   â”œâ”€â”€ addition_coupled.yaml
â”‚   â”‚   â”œâ”€â”€ addition_index_hint.yaml
â”‚   â”‚   â””â”€â”€ ... other task configs ...
â”‚   â””â”€â”€ training/
â”‚       â””â”€â”€ default.yaml
â”œâ”€â”€ dataset/ (generated by running code)
â”œâ”€â”€ log/     (generated by running code)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ addition/
â”‚   â”‚   â”œâ”€â”€ run_<METHOD>.sh
â”‚   â”‚   â”œâ”€â”€ eval_<METHOD>.sh
â”‚   â”‚   â””â”€â”€ attn_mtx.sh
â”‚   â”œâ”€â”€ Nx2multiplication/
â”‚   â”‚   â”œâ”€â”€ run_<METHOD>.sh
â”‚   â”‚   â””â”€â”€ eval_<METHOD>.sh
â”‚   â””â”€â”€ ... other folders of script files for other tasks ...
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ training_utils.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ arithmetic_dataset.py   (build dataset here)
â”‚   â”‚   â”œâ”€â”€ common.py               (Parent class `ArithmeticDataset`)
â”‚   â”‚   â””â”€â”€ <TASK_NAME>.py          (addition, multiplication, ...)
â”‚   â”œâ”€â”€ evaluate/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ accuracy.py
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ build_model.py
â”‚   â”‚   â””â”€â”€ modeling/
â”‚   â”‚       â”œâ”€â”€ custom_gpt2.py
â”‚   â”‚       â”œâ”€â”€ custom_t5_decoder_only.py   (our main model)
â”‚   â”‚       â””â”€â”€ positional_embeddings.py
â”‚   â”œâ”€â”€ tokenization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tokenization.py
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ optimization.py
â”œâ”€â”€ vis/  (make it yourself, for visualization)
â””â”€â”€ wandb/  (automatically generated when using W&B)
```


