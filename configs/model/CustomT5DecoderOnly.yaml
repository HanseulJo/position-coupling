model_name: CustomT5DecoderOnly

position_encoding_type: abs_learned 
# Example of position_encoding_type:
# "t5_relative_bias"
# "transformer_xl_relative_encoding"
# "rotary"
# "rotary_rerun"
# "new_rotary"
# "abs_learned"
# "abs_sinusoid"
# "alibi"
# "alibi_learned"
# "none"

## Dimensions ##
d_model: 1024
d_kv: 128
d_ff: 2048
num_layers: 6
num_decoder_layers: null
num_heads: 8
n_positions: 1024

## Multi-level Position ID supports $##
d_positions: null
share_pe: null

## Relative Biases ##
relative_attention_num_buckets: null
relative_attention_max_distance: null
logarithmic_rel_bias_scale_base: null

## RoPE ##
rope_theta: null
partial_rotary_factor: null
rope_head_dim: null
rope_scaling:
  factor: null
  attention_factor: null
  beta_fast: null
  beta_slow: null

## Architecture Details ##
dropout_rate: 0.0
classifier_dropout: 0.0
normalization_layer: rmsnorm  # layernorm, t5layernorm, rmsnorm
layer_norm_position: pre_post  # pre, post, pre_post
layer_norm_epsilon: 0.000001
initializer_factor: 1.0
feed_forward_proj: gated-gelu  # relu, gelu, gated-relu, gated-gelu
is_encoder_decoder: False
use_cache: True

## Tempered Softmax ##
tempered_softmax: False
tempered_softmax_std: null

## Common config ##
do_sample: False
num_beams: 1
# early_stopping: False

## Save/Compile model or not ##
save: True
compile: True