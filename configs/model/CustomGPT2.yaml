model_name: CustomGPT2

# attribute_map = {
#     "hidden_size": "n_embd",
#     "max_position_embeddings": "n_positions",
#     "num_attention_heads": "n_head",
#     "num_hidden_layers": "n_layer",
# }

n_positions: 2048
d_embd: 768
d_kv: 64
n_inner: 3072
n_layer: 12
n_head: 12
activation_function: relu
resid_pdrop: 0.
embd_pdrop: 0.
attn_pdrop: 0.
layer_norm_epsilon: 0.00001
initializer_range: 0.02
scale_attn_weights: True
use_cache: True
scale_attn_by_inverse_layer_idx: False
reorder_and_upcast_attn: False
normalization_layer: layernorm
normalization_position: pre
position_encoding_type: abs_learned


do_sample: False
num_beams: 1
save: True