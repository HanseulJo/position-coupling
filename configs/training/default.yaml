batch_size_train: 100
batch_size_eval: 100
num_workers: 0

grad_clip: 1.0

n_steps: 50000
calc_acc_every_epochs: 1
log_every_steps: 100

optimizer:
  type: AdamW
  lr: 0.0001
  weight_decay: 0.
  betas: [0.9, 0.95]
  eps: 0.00000001

scheduler: 
  type: custom_cosine
  warmup_ratio: 0.01
  min_lr_ratio: 0.1

logger:
  print_every: 1